# hyperparameters for the PyTorch Vision Transformer example

# Model parameters
patch_size: 16           # patch size for images
latent_size: 768
n_channels: 3            # RGB
num_heads: 12
num_encoders: 12
dropout: 0.1
img_size: 224            # image size to be reshaped to
num_classes: 16          # number of classes in dataset

# Training configuration
epochs: 10               # number of training epochs
lr: 0.01                 # base learning rate
weight_decay: 0.03       # weight decay for Adam
global_batch_size: 4